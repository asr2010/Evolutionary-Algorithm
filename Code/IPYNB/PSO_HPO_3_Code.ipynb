{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38011459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pyswarms as ps\n",
    "from functools import partial\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "print(\"Loading and preprocessing CIFAR-10 dataset\")\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_train, y_test = tf.keras.utils.to_categorical(y_train), tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "print(\"Splitting the training data into train and validation sets\")\n",
    "validation_split = 0.1\n",
    "split_index = int(len(x_train) * validation_split)\n",
    "x_val, y_val = x_train[:split_index], y_train[:split_index]\n",
    "x_train, y_train = x_train[split_index:], y_train[split_index:]\n",
    "\n",
    "sample_size = 1000  # Adjust this value as needed\n",
    "sample_indices = np.random.choice(np.arange(x_train.shape[0]), sample_size, replace=False)\n",
    "\n",
    "x_train_small = x_train[sample_indices]\n",
    "y_train_small = y_train[sample_indices]\n",
    "\n",
    "split_index_small = int(len(x_train_small) * validation_split)\n",
    "x_val_small, y_val_small = x_train_small[:split_index_small], y_train_small[:split_index_small]\n",
    "x_train_small, y_train_small = x_train_small[split_index_small:], y_train_small[split_index_small:]\n",
    "\n",
    "# Define a fitness function to be optimized using PSO\n",
    "def fitness_function(hparams, x_train, y_train, x_val, y_val):\n",
    "    fitness_values = []\n",
    "    for hparam in hparams:\n",
    "        num_filters1, num_filters2, dense_units, learning_rate = hparam\n",
    "        num_filters1 = int(num_filters1)\n",
    "        num_filters2 = int(num_filters2)\n",
    "        dense_units = int(dense_units)\n",
    "        \n",
    "        print(f\"Hyperparameters: num_filters1={num_filters1}, \"\n",
    "              f\"num_filters2={num_filters2}, dense_units={dense_units}, \"\n",
    "              f\"learning_rate={learning_rate}\")\n",
    "\n",
    "        model = Sequential([\n",
    "            Conv2D(num_filters1, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "            BatchNormalization(),\n",
    "            Conv2D(num_filters1, (3, 3), activation='relu', padding='same'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Dropout(0.25),\n",
    "\n",
    "            Conv2D(num_filters2, (3, 3), activation='relu', padding='same'),\n",
    "            BatchNormalization(),\n",
    "            Conv2D(num_filters2, (3, 3), activation='relu', padding='same'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Dropout(0.25),\n",
    "\n",
    "            Flatten(),\n",
    "            Dense(dense_units, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.5),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "        history = model.fit(x_train, y_train, epochs=10, batch_size=64,\n",
    "                            validation_data=(x_val, y_val),\n",
    "                            callbacks=[early_stopping],\n",
    "                            verbose=0)\n",
    "\n",
    "        best_val_acc = max(history.history['val_accuracy'])\n",
    "        fitness_values.append(1 - best_val_acc)  # Minimize the fitness function (1 - val_accuracy)\n",
    "\n",
    "    return np.array(fitness_values)\n",
    "\n",
    "# Define the PSO search space for hyperparameters\n",
    "print(\"Defining the PSO search space for hyperparameters\")\n",
    "search_space_bounds = (np.array([16, 16, 128, 1e-5]),\n",
    "                       np.array([128, 128, 1024, 1e-2]))\n",
    "\n",
    "# Define the fitness function with fixed data arguments\n",
    "print(\"Defining the fitness function with fixed data arguments\")\n",
    "fitness_function_data = partial(fitness_function,\n",
    "                                x_train=x_train_small, y_train=y_train_small,\n",
    "                                x_val=x_val_small, y_val=y_val_small)\n",
    "\n",
    "# Initialize the PSO optimizer\n",
    "print(\"Initializing the PSO optimizer\")\n",
    "options = {'c1': 1.0, 'c2': 1.9, 'w': 0.8}\n",
    "optimizer = ps.single.GlobalBestPSO(n_particles=20, dimensions=4, options=options,\n",
    "                                    bounds=search_space_bounds)\n",
    "\n",
    "# Run the PSO optimizer\n",
    "print(\"Running the PSO optimizer\")\n",
    "cost, best_hyperparams = optimizer.optimize(fitness_function_data, iters=20)\n",
    "\n",
    "# Extract the best hyperparameters\n",
    "best_num_filters1, best_num_filters2, best_dense_units, best_learning_rate = best_hyperparams\n",
    "best_num_filters1 = int(best_num_filters1)\n",
    "best_num_filters2 = int(best_num_filters2)\n",
    "best_dense_units = int(best_dense_units)\n",
    "\n",
    "print(f\"Best hyperparameters found by PSO: num_filters1={best_num_filters1}, \"\n",
    "      f\"num_filters2={best_num_filters2}, dense_units={best_dense_units}, \"\n",
    "      f\"learning_rate={best_learning_rate}\")\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "print(\"Training the model with the best hyperparameters\")\n",
    "model = Sequential([\n",
    "    Conv2D(best_num_filters1, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(best_num_filters1, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv2D(best_num_filters2, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(best_num_filters2, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(best_dense_units, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=best_learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=50, batch_size=64,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=1)\n",
    "end_time = time.time()\n",
    "\n",
    "training_time = end_time - start_time\n",
    "print(f'Total training time: {training_time:.2f} seconds')\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "print(\"Evaluating the model on the test dataset\")\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_classes, y_pred_classes))\n",
    "\n",
    "# Calculate ROC-AUC for multi-class classification\n",
    "roc_auc = roc_auc_score(y_test, y_pred, multi_class='ovr')\n",
    "print(f'ROC-AUC Score: {roc_auc}')\n",
    "\n",
    "# Plot the training and validation accuracies\n",
    "print(\"Plotting the training and validation accuracies\")\n",
    "plt.plot(history.history['accuracy'], label='Training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1f4ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using these hardcoded values as obtained from confusion matrix above inorder to make a plot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Confusion matrix data\n",
    "confusion_matrix = np.array([\n",
    "    [788, 9, 60, 20, 5, 3, 4, 9, 73, 29],\n",
    "    [11, 879, 4, 2, 2, 3, 4, 1, 30, 64],\n",
    "    [41, 1, 723, 52, 75, 32, 44, 15, 10, 7],\n",
    "    [9, 1, 64, 658, 53, 131, 45, 16, 12, 11],\n",
    "    [5, 0, 43, 47, 836, 19, 21, 22, 7, 0],\n",
    "    [8, 1, 37, 145, 36, 730, 9, 27, 5, 2],\n",
    "    [3, 2, 45, 42, 27, 5, 864, 4, 7, 1],\n",
    "    [6, 0, 17, 34, 49, 29, 2, 853, 2, 8],\n",
    "    [33, 5, 7, 8, 5, 3, 3, 0, 922, 14],\n",
    "    [19, 33, 5, 10, 5, 2, 2, 4, 20, 900]\n",
    "])\n",
    "\n",
    "# Class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
