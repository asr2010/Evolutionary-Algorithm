{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0f9c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from deap import base, creator, tools\n",
    "from functools import partial\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import time\n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_train, y_test = tf.keras.utils.to_categorical(y_train), tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "validation_split = 0.1\n",
    "split_index = int(len(x_train) * validation_split)\n",
    "x_val, y_val = x_train[:split_index], y_train[:split_index]\n",
    "x_train, y_train = x_train[split_index:], y_train[split_index:]\n",
    "\n",
    "sample_size = 5000  # Adjust this value as needed\n",
    "sample_indices = np.random.choice(np.arange(x_train.shape[0]), sample_size, replace=False)\n",
    "\n",
    "x_train_small = x_train[sample_indices]\n",
    "y_train_small = y_train[sample_indices]\n",
    "\n",
    "# Create fitness and individual classes for DEAP\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "# Initialize the individual and population functions for DEAP\n",
    "def init_individual():\n",
    "    num_filters1 = random.randint(16, 128)\n",
    "    num_filters2 = random.randint(16, 128)\n",
    "    dense_units = random.randint(128, 1024)\n",
    "    learning_rate = random.uniform(1e-5, 1e-2)\n",
    "    return creator.Individual([num_filters1, num_filters2, dense_units, learning_rate])\n",
    "\n",
    "def init_population(n):\n",
    "    return [init_individual() for _ in range(n)]\n",
    "\n",
    "def fitness_function_data(individual):\n",
    "    num_filters1, num_filters2, dense_units, learning_rate = individual\n",
    "    num_filters1 = int(num_filters1)\n",
    "    num_filters2 = int(num_filters2)\n",
    "    dense_units = int(dense_units)\n",
    "    \n",
    "    print(f\"Training with hyperparameters: num_filters1={num_filters1}, num_filters2={num_filters2}, \"\n",
    "          f\"dense_units={dense_units}, learning_rate={learning_rate}\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv2D(num_filters1, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(num_filters1, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Conv2D(num_filters2, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(num_filters2, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(x_train_small, y_train_small, epochs=5, batch_size=256,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=0)\n",
    "    \n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    return val_loss,\n",
    "\n",
    "# Define the evaluation, crossover, and mutation functions for DEAP\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"individual\", init_individual)\n",
    "toolbox.register(\"population\", init_population, n=10)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.2, indpb=0.1)\n",
    "toolbox.register(\"select\", tools.selBest)\n",
    "toolbox.register(\"evaluate\", fitness_function_data)\n",
    "\n",
    "# Run the GA optimizer\n",
    "population = toolbox.population()\n",
    "NGEN = 20\n",
    "\n",
    "for gen in range(NGEN):\n",
    "    print(f\"Generation {gen + 1} of {NGEN}\")\n",
    "    offspring = tools.selBest(population, len(population) // 2)\n",
    "    offspring = list(offspring)\n",
    "    for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "        if random.random() < 0.5:\n",
    "            toolbox.mate(child1, child2)\n",
    "            del child1.fitness.values\n",
    "            del child2.fitness.values\n",
    "\n",
    "    for mutant in offspring:\n",
    "        if random.random() < 0.2:\n",
    "            toolbox.mutate(mutant)\n",
    "            del mutant.fitness.values\n",
    "\n",
    "    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "    fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    # Sort the population based on their fitness values\n",
    "    population = sorted(population, key=lambda ind: ind.fitness.values[0] if ind.fitness.valid else float('inf'))\n",
    "\n",
    "    # Replace the least fit individuals with offspring\n",
    "    population[:len(offspring)] = offspring\n",
    "    \n",
    "    # Print the best fitness value in the current generation\n",
    "    best_fitness = min(ind.fitness.values[0] for ind in population if ind.fitness.valid)\n",
    "    print(f\"Best fitness in generation {gen + 1}: {best_fitness:.4f}\")\n",
    "\n",
    "best_individual = tools.selBest(population, 1)[0]\n",
    "\n",
    "# Extract the best hyperparameters\n",
    "best_num_filters1, best_num_filters2, best_dense_units, best_learning_rate = best_individual\n",
    "best_num_filters1 = int(best_num_filters1)\n",
    "best_num_filters2 = int(best_num_filters2)\n",
    "best_dense_units = int(best_dense_units)\n",
    "\n",
    "\n",
    "print(f\"Best hyperparameters found by GA: num_filters1={best_num_filters1}, \"\n",
    "      f\"num_filters2={best_num_filters2}, dense_units={best_dense_units}, \"\n",
    "      f\"learning_rate={best_learning_rate}\")\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "model = Sequential([\n",
    "    Conv2D(best_num_filters1, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(best_num_filters1, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv2D(best_num_filters2, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(best_num_filters2, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(best_dense_units, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=best_learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               patience=5, restore_best_weights=True)\n",
    "\n",
    "start_time = time.time()\n",
    "history = model.fit(x_train, y_train, epochs=50, batch_size=64,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=1)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "training_time = end_time - start_time\n",
    "print(f'Total training time: {training_time:.2f} seconds')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_classes, y_pred_classes))\n",
    "\n",
    "# Calculate ROC-AUC for multi-class classification\n",
    "roc_auc = roc_auc_score(y_test, y_pred, multi_class='ovr')\n",
    "print(f'ROC-AUC Score: {roc_auc}')\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77152f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using these hardcoded values as obtained from confusion matrix above inorder to make a plot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Confusion matrix data\n",
    "confusion_matrix = np.array([\n",
    "    [805, 18, 31, 31, 7, 4, 6, 15, 52, 31],\n",
    "    [5, 933, 1, 2, 1, 1, 3, 1, 11, 42],\n",
    "    [55, 5, 621, 88, 66, 53, 56, 37, 12, 7],\n",
    "    [13, 5, 29, 695, 33, 139, 51, 25, 6, 4],\n",
    "    [18, 3, 26, 65, 721, 55, 52, 51, 4, 5],\n",
    "    [6, 2, 10, 144, 22, 773, 8, 32, 1, 2],\n",
    "    [4, 2, 18, 54, 17, 25, 868, 8, 2, 2],\n",
    "    [10, 1, 5, 32, 22, 58, 3, 859, 1, 9],\n",
    "    [41, 19, 2, 11, 1, 5, 4, 4, 898, 15],\n",
    "    [15, 57, 3, 11, 0, 5, 2, 9, 18, 880]\n",
    "])\n",
    "\n",
    "# Class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
